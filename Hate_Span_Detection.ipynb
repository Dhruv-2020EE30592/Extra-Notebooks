{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 67279,
          "databundleVersionId": 7469949,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30674,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dhruv-2020EE30592/Extra-Notebooks/blob/main/Hate_Span_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from transformers import DistilBertTokenizerFast, DistilBertModel, AdamW"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-07T16:41:09.293331Z",
          "iopub.execute_input": "2024-05-07T16:41:09.293993Z",
          "iopub.status.idle": "2024-05-07T16:41:09.29967Z",
          "shell.execute_reply.started": "2024-05-07T16:41:09.29396Z",
          "shell.execute_reply": "2024-05-07T16:41:09.298643Z"
        },
        "trusted": true,
        "id": "OoDmmyWzbg7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-07T16:41:09.499165Z",
          "iopub.execute_input": "2024-05-07T16:41:09.499427Z",
          "iopub.status.idle": "2024-05-07T16:41:09.504598Z",
          "shell.execute_reply.started": "2024-05-07T16:41:09.499405Z",
          "shell.execute_reply": "2024-05-07T16:41:09.503695Z"
        },
        "trusted": true,
        "id": "EVb1Mw-sbg7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-07T16:41:09.658082Z",
          "iopub.execute_input": "2024-05-07T16:41:09.658347Z",
          "iopub.status.idle": "2024-05-07T16:41:09.675191Z",
          "shell.execute_reply.started": "2024-05-07T16:41:09.658325Z",
          "shell.execute_reply": "2024-05-07T16:41:09.674367Z"
        },
        "trusted": true,
        "id": "vWTrmNBmbg7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/kaggle/input/hatenorm/train.csv', delimiter='|')\n",
        "df_test = pd.read_csv('/kaggle/input/hatenorm/test.csv', delimiter='|')\n",
        "df.head(1).sentence"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-07T16:41:09.859906Z",
          "iopub.execute_input": "2024-05-07T16:41:09.860689Z",
          "iopub.status.idle": "2024-05-07T16:41:09.885067Z",
          "shell.execute_reply.started": "2024-05-07T16:41:09.860654Z",
          "shell.execute_reply": "2024-05-07T16:41:09.884231Z"
        },
        "trusted": true,
        "id": "AqpcmKSubg7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def remove_special_characters(text):\n",
        "#     cleaned_text = ''\n",
        "#     for char in text:\n",
        "#         if char.isalnum() or char==' ':\n",
        "#             cleaned_text += char\n",
        "#     return cleaned_text\n",
        "\n",
        "# df['sentence'] = df['sentence'].apply(lambda x: remove_special_characters(x))\n",
        "# df_test['sentence'] = df_test['sentence'].apply(lambda x: remove_special_characters(x))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-07T16:41:10.048277Z",
          "iopub.execute_input": "2024-05-07T16:41:10.048525Z",
          "iopub.status.idle": "2024-05-07T16:41:10.052528Z",
          "shell.execute_reply.started": "2024-05-07T16:41:10.048503Z",
          "shell.execute_reply": "2024-05-07T16:41:10.051601Z"
        },
        "trusted": true,
        "id": "E1sbsu60bg7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset_testing(Dataset):\n",
        "\n",
        "    def __init__(self, dataframe):\n",
        "        self.data = dataframe\n",
        "        self.input = self.data.sentence\n",
        "        self.max_len = 512\n",
        "        self.Id = self.data.Id\n",
        "        self.tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            self.input.iloc[index],\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "        tag_to_idx = {\"B\":0, \"I\":1, \"O\":2}\n",
        "\n",
        "        ids = (encoding['input_ids'])\n",
        "        mask = (encoding['attention_mask'])\n",
        "        word_to_token_len_dict =  [-1 for _ in range(512)]\n",
        "\n",
        "        tokens = (self.tokenizer.convert_ids_to_tokens(encoding['input_ids']))\n",
        "        token_type_ids = (encoding['token_type_ids'])\n",
        "        original_sentence = self.input.iloc[index].split()\n",
        "        start_ind = 1\n",
        "        end_ind = 1\n",
        "\n",
        "        for ind in range(len(original_sentence)):\n",
        "            word_encoding = self.tokenizer.encode_plus(\n",
        "                original_sentence[ind],\n",
        "                max_length=self.max_len,\n",
        "                padding=False,\n",
        "                truncation=True\n",
        "            )\n",
        "\n",
        "            num_of_word_tokens = len(word_encoding['input_ids'])-2\n",
        "            word_to_token_len_dict[ind] = num_of_word_tokens\n",
        "#             print(num_of_word_tokens, \" \",original_sentence[ind],  \"word_encodings: \", word_encoding)\n",
        "            end_ind = start_ind + num_of_word_tokens\n",
        "            start_ind = end_ind\n",
        "\n",
        "        return {\n",
        "            'Id':torch.tensor(self.Id.iloc[index]),\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'tokens': tokens,\n",
        "            'word_to_token_len_dict': torch.tensor(word_to_token_len_dict, dtype=torch.long)\n",
        "\n",
        "        }"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-07T16:41:10.22529Z",
          "iopub.execute_input": "2024-05-07T16:41:10.225739Z",
          "iopub.status.idle": "2024-05-07T16:41:10.237509Z",
          "shell.execute_reply.started": "2024-05-07T16:41:10.225714Z",
          "shell.execute_reply": "2024-05-07T16:41:10.236655Z"
        },
        "trusted": true,
        "id": "Bn_mycxvbg7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataframe):\n",
        "        self.data = dataframe\n",
        "        self.Id = self.data.Id\n",
        "        self.input = self.data.sentence\n",
        "        self.output = self.data.bio\n",
        "        self.max_len = 512\n",
        "        self.tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            self.input.iloc[index],\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "        tag_to_idx = {\"B\":0, \"I\":1, \"O\":2}\n",
        "\n",
        "        ids = (encoding['input_ids'])\n",
        "        mask = (encoding['attention_mask'])\n",
        "        tokens = (self.tokenizer.convert_ids_to_tokens(encoding['input_ids']))\n",
        "        token_type_ids = (encoding['token_type_ids'])\n",
        "        targets = [tag_to_idx[tag] for tag in self.output.iloc[index].replace(\" \", \"\")]\n",
        "        custom_targets = [[0, 1] for _ in range(len(encoding['input_ids']))]\n",
        "        custom_targets_tags_only = [2 for _ in range(len(encoding['input_ids']))]\n",
        "        word_to_token_len_dict = [-1 for _ in range(512)]\n",
        "        original_sentence = self.input.iloc[index].split()\n",
        "        start_ind = 1\n",
        "        end_ind = 1\n",
        "\n",
        "        for ind in range(len(original_sentence)):\n",
        "            word_encoding = self.tokenizer.encode_plus(\n",
        "                original_sentence[ind],\n",
        "                max_length=self.max_len,\n",
        "                padding=False,\n",
        "                truncation=True\n",
        "            )\n",
        "            num_of_word_tokens = len(word_encoding['input_ids'])-2\n",
        "            word_to_token_len_dict[ind] = num_of_word_tokens\n",
        "#             print(num_of_word_tokens, \" \",original_sentence[ind],  \"word_encodings: \", word_encoding)\n",
        "            end_ind = start_ind + num_of_word_tokens\n",
        "            try:\n",
        "                if(targets[ind] == 0):\n",
        "                    curr_cusotm_targets = [[1, 0] for _ in range(num_of_word_tokens)]\n",
        "                    curr_cusotm_targets_tags_only = [0 for _ in range(num_of_word_tokens)]\n",
        "#                     curr_cusotm_targets[0] = [1, 0, 0]\n",
        "#                     curr_cusotm_targets_tags_only[0] = 0\n",
        "\n",
        "                    custom_targets[start_ind:end_ind] = (curr_cusotm_targets)\n",
        "                    custom_targets_tags_only[start_ind:end_ind] = curr_cusotm_targets_tags_only\n",
        "\n",
        "\n",
        "                elif(targets[ind] == 1):\n",
        "                    curr_cusotm_targets = [[1, 0] for _ in range(num_of_word_tokens)]\n",
        "                    curr_cusotm_targets_tags_only = [0 for _ in range(num_of_word_tokens)]\n",
        "                    custom_targets[start_ind:end_ind] = (curr_cusotm_targets)\n",
        "                    custom_targets_tags_only[start_ind:end_ind] = curr_cusotm_targets_tags_only\n",
        "\n",
        "                else:\n",
        "                    curr_cusotm_targets = [[0, 1] for _ in range(num_of_word_tokens)]\n",
        "                    curr_cusotm_targets_tags_only = [1 for _ in range(num_of_word_tokens)]\n",
        "                    custom_targets[start_ind:end_ind] = (curr_cusotm_targets)\n",
        "                    custom_targets_tags_only[start_ind:end_ind] = curr_cusotm_targets_tags_only\n",
        "            except:\n",
        "                print(f'Error occured for input: { self.input.iloc[index]}')\n",
        "                print(\" \")\n",
        "                print(f'The original_sentence: {original_sentence}')\n",
        "                print(\" \")\n",
        "                print(f'targets: {targets}')\n",
        "                print(\" \")\n",
        "                print(f'ids: {ids}')\n",
        "                print(\" \")\n",
        "                print(f'Error index: {ind} and {original_sentence[ind]}' )\n",
        "#             print(\"Custom Targets: \", custom_targets)\n",
        "            start_ind = end_ind\n",
        "\n",
        "        return {\n",
        "            'Id':torch.tensor(self.Id.iloc[index]),\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'tokens': tokens,\n",
        "            'targets': torch.tensor(custom_targets, dtype=torch.long),\n",
        "            'word_to_token_len_dict': torch.tensor(word_to_token_len_dict, dtype=torch.long),\n",
        "            'custom_targets_tags_only': torch.tensor(custom_targets_tags_only, dtype=torch.long)\n",
        "#                 print(f'Error in creating tensor for target {custom_target}')\n",
        "        }"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-07T16:42:51.984707Z",
          "iopub.execute_input": "2024-05-07T16:42:51.985638Z",
          "iopub.status.idle": "2024-05-07T16:42:52.00554Z",
          "shell.execute_reply.started": "2024-05-07T16:42:51.985604Z",
          "shell.execute_reply": "2024-05-07T16:42:52.004647Z"
        },
        "trusted": true,
        "id": "8JfhKVDsbg7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import BertTokenizer\n",
        "\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# demo_data = {'Id':[1, 2], 'sentence': ['Hello', 'Say it loud , say it clear , illegal #immigrants are not welcome here @user'],\n",
        "#         'bio': ['0', 'O O O O O O O O B I O O O O O']}\n",
        "# demo_df = pd.DataFrame(demo_data)\n",
        "\n",
        "# # Creating an instance of the dataset\n",
        "# dataset = CustomDataset(demo_df)\n",
        "# sentence = (dataset[1]['ids'])\n",
        "# print((dataset[1]))\n",
        "# print((dataset[1]['targets']))\n",
        "# print((dataset[1]['ids']))\n",
        "# print((dataset[1]['targets']))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-07T16:42:52.517585Z",
          "iopub.execute_input": "2024-05-07T16:42:52.517917Z",
          "iopub.status.idle": "2024-05-07T16:42:52.522073Z",
          "shell.execute_reply.started": "2024-05-07T16:42:52.517888Z",
          "shell.execute_reply": "2024-05-07T16:42:52.521234Z"
        },
        "trusted": true,
        "id": "7_OX8M5lbg7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, val_data = train_test_split(df, test_size=0.3, random_state=0)\n",
        "print(len(train_data))\n",
        "train_dataset = CustomDataset(train_data)\n",
        "val_dataset = CustomDataset(val_data)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-07T16:42:52.886263Z",
          "iopub.execute_input": "2024-05-07T16:42:52.886602Z",
          "iopub.status.idle": "2024-05-07T16:42:53.14887Z",
          "shell.execute_reply.started": "2024-05-07T16:42:52.886572Z",
          "shell.execute_reply": "2024-05-07T16:42:53.148149Z"
        },
        "trusted": true,
        "id": "Eo3Pjcu0bg7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-07T16:42:53.382605Z",
          "iopub.execute_input": "2024-05-07T16:42:53.383439Z",
          "iopub.status.idle": "2024-05-07T16:42:53.394844Z",
          "shell.execute_reply.started": "2024-05-07T16:42:53.383398Z",
          "shell.execute_reply": "2024-05-07T16:42:53.393703Z"
        },
        "trusted": true,
        "id": "CRDOJWn1bg7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = CustomDataset_testing(df_test)\n",
        "test_dataloader = DataLoader(test_dataset, shuffle=False)\n",
        "# print((test_dataset[0]))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-07T16:42:54.106362Z",
          "iopub.execute_input": "2024-05-07T16:42:54.106883Z",
          "iopub.status.idle": "2024-05-07T16:42:54.251226Z",
          "shell.execute_reply.started": "2024-05-07T16:42:54.106841Z",
          "shell.execute_reply": "2024-05-07T16:42:54.250262Z"
        },
        "trusted": true,
        "id": "gHF-gr-Zbg7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print((train_dataset[0]))\n",
        "# print(str(train_data.head(1).bio))\n",
        "# print(len(train_dataset[0]['ids']))\n",
        "# print((train_dataset[1]['word_to_token_len_dict']))\n",
        "# print(len(train_dataset[0]['token_type_ids']))\n",
        "# print(len(train_dataset[0]['tokens']))\n",
        "# print((train_dataset[0]['targets']))\n",
        "\n",
        "# print(train_dataloader[0].shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-07T16:42:54.634661Z",
          "iopub.execute_input": "2024-05-07T16:42:54.635276Z",
          "iopub.status.idle": "2024-05-07T16:42:54.640681Z",
          "shell.execute_reply.started": "2024-05-07T16:42:54.635245Z",
          "shell.execute_reply": "2024-05-07T16:42:54.639805Z"
        },
        "trusted": true,
        "id": "6-Mi-rzBbg7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(len(str(df.head(1).sentence).split(\" \")))\n",
        "# print((str(df.head(1).sentence).split(\" \")))\n",
        "# print(len(str(df.head(1).bio).split(\" \")))\n",
        "# print((str(df.head(1).bio).split(\" \")))\n",
        "# # print(len(val_data))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-07T16:42:55.098247Z",
          "iopub.execute_input": "2024-05-07T16:42:55.099013Z",
          "iopub.status.idle": "2024-05-07T16:42:55.102564Z",
          "shell.execute_reply.started": "2024-05-07T16:42:55.098985Z",
          "shell.execute_reply": "2024-05-07T16:42:55.101643Z"
        },
        "trusted": true,
        "id": "cB-2cOO4bg7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(outputs, targets):\n",
        "    return torch.nn.CrossEntropyLoss()(outputs, targets)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-07T16:42:55.464631Z",
          "iopub.execute_input": "2024-05-07T16:42:55.464967Z",
          "iopub.status.idle": "2024-05-07T16:42:55.469538Z",
          "shell.execute_reply.started": "2024-05-07T16:42:55.464941Z",
          "shell.execute_reply": "2024-05-07T16:42:55.468537Z"
        },
        "trusted": true,
        "id": "KAoMAwLYbg7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "def customise_output(output):\n",
        "    try:\n",
        "        _, class_indices = torch.max(output, dim=2)\n",
        "        return class_indices\n",
        "    except:\n",
        "        print(f'Dimenation mismatch: the output shuold be of the dimention torch.tensor([{batch_size}, 512, 3])')\n",
        "\n",
        "class CustomModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.l1 = DistilBertModel.from_pretrained('distilbert-base-uncased', output_attentions=False, output_hidden_states=True)\n",
        "        self.l2 = torch.nn.Dropout(0.3)\n",
        "        self.l3 = torch.nn.Linear(768, 2)\n",
        "\n",
        "    def forward(self, ids, mask):\n",
        "        _, output_1= self.l1(ids, attention_mask = mask, return_dict=False)\n",
        "#         print(\"Outout from bert layer: \", _)\n",
        "#         print(\"Output: \", output_1)\n",
        "        output_2 = self.l2(_)\n",
        "        output = self.l3(output_2)\n",
        "        class_indices = customise_output(output)\n",
        "\n",
        "#         probabilities = F.softmax(output, dim=-1)  # Apply softmax on the last dimension to convert logits to probabilities\n",
        "        return output, class_indices\n",
        "\n",
        "model = CustomModel()\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-05)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-07T16:42:55.790061Z",
          "iopub.execute_input": "2024-05-07T16:42:55.79033Z",
          "iopub.status.idle": "2024-05-07T16:42:56.126817Z",
          "shell.execute_reply.started": "2024-05-07T16:42:55.790307Z",
          "shell.execute_reply": "2024-05-07T16:42:56.12587Z"
        },
        "trusted": true,
        "id": "_GnEvBw2bg7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch, num_epochs):\n",
        "    model.train()\n",
        "    train_loop = tqdm(train_dataloader, desc=f'Epoch {epoch+1}/{num_epochs}, Training')\n",
        "#     print(enumerate(train_loop))\n",
        "    for _,data in enumerate(train_loop):\n",
        "        ids = data['ids'].to(device, dtype = torch.long)\n",
        "        mask = data['mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.float)\n",
        "        outputs, class_indices = model(ids, mask)\n",
        "#         print(\"outputs shape: \", outputs.shape, \" \", outputs[0])\n",
        "#         print(\"targets shape: \", targets.shape,  \" \", targets[0])\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "def checkpoint(epoch):\n",
        "    checkpoint_filename = f\"checkpoint_epoch_{epoch+1}.pth\"\n",
        "    torch.save({\n",
        "        'epoch': epoch+1,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "    }, checkpoint_filename)\n",
        "\n",
        "def validation(epoch, num_epochs):\n",
        "    model.eval()\n",
        "    val_loop = tqdm(val_dataloader, desc=f'Epoch {epoch+1}/{num_epochs}, Validation')\n",
        "    fin_targets=[]\n",
        "    fin_outputs=[]\n",
        "\n",
        "    #bio tags outputs for tokenised sentence\n",
        "    fin_bio_outputs=[]\n",
        "    fin_targets_one_hot_encoded = []\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(val_loop):\n",
        "            ids = data['ids'].to(device, dtype = torch.long)\n",
        "            mask = data['mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "            targets_one_hot_encoded = data['targets'].to(device, dtype = torch.float)\n",
        "            targets = data['custom_targets_tags_only'].to(device, dtype = torch.float)\n",
        "            outputs, classfication_indices = model(ids, mask)\n",
        "            fin_targets_one_hot_encoded.extend(targets_one_hot_encoded.cpu().detach().numpy().tolist())\n",
        "            fin_bio_outputs.extend(classfication_indices)\n",
        "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
        "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "    return fin_outputs, fin_targets, fin_bio_outputs, fin_targets_one_hot_encoded\n",
        "\n",
        "def convert_islands(tags):\n",
        "    # Create a copy of the list to avoid modifying the input list directly\n",
        "    converted_tags = tags[:]\n",
        "\n",
        "    # Use enumerate to get both index and value in the list\n",
        "    for i, tag in enumerate(converted_tags):\n",
        "        # Check if current tag is 'I'\n",
        "        if tag == 'I':\n",
        "            # Change it to 'B' if it's the start of the list or the previous tag is not 'I'\n",
        "            if i == 0 or tags[i - 1] != 'I':\n",
        "                converted_tags[i] = 'B'\n",
        "\n",
        "    return converted_tags\n",
        "\n",
        "def testing():\n",
        "    model.eval()\n",
        "    val_loop = tqdm(test_dataloader, desc=f'Testing')\n",
        "    fin_targets=[]\n",
        "    fin_outputs=[]\n",
        "    submission = {}\n",
        "    submission_2 = {}\n",
        "    #bio tags outputs for tokenised sentence\n",
        "    fin_bio_outputs=[]\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(val_loop):\n",
        "            Id = data['Id'].to(device, dtype = torch.long)\n",
        "            ids = data['ids'].to(device, dtype = torch.long)\n",
        "            mask = data['mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "#             targets_one_hot_encoded = data['targets'].to(device, dtype = torch.float)\n",
        "#             targets = data['custom_targets_tags_only'].to(device, dtype = torch.float)\n",
        "            word_to_token_len_dict = data['word_to_token_len_dict']\n",
        "            outputs, classfication_indices = model(ids, mask)\n",
        "            start_ind = 0\n",
        "            final_tags = []\n",
        "#             print(classfication_indices.shape)\n",
        "            word_to_token_len_dict_int_list = [t.item() for t in word_to_token_len_dict[0]]\n",
        "            #convert back output tokens to sentence length of test dataset\n",
        "            for token_len in word_to_token_len_dict_int_list:\n",
        "                if token_len != -1:\n",
        "                    word_tags = [t.item() for t in classfication_indices[0][start_ind:start_ind+token_len]]\n",
        "                    if 0 in word_tags:\n",
        "                        final_tags.append('I')\n",
        "                    else:\n",
        "                        final_tags.append('O')\n",
        "                    start_ind += token_len\n",
        "\n",
        "            submission_2[Id[0].item()] = final_tags\n",
        "            final_tags = convert_islands(final_tags)\n",
        "\n",
        "            final_output_string = ' '.join(final_tags)\n",
        "            submission[Id[0].item()] = final_output_string\n",
        "\n",
        "            #bio tags outputs for tokenised sentence\n",
        "#             fin_targets_one_hot_encoded.extend(targets_one_hot_encoded.cpu().detach().numpy().tolist())\n",
        "            fin_bio_outputs.extend(classfication_indices)\n",
        "#             fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
        "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "\n",
        "    return fin_outputs, fin_targets, fin_bio_outputs, submission, submission_2"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-07T16:52:19.555144Z",
          "iopub.execute_input": "2024-05-07T16:52:19.555501Z",
          "iopub.status.idle": "2024-05-07T16:52:19.578906Z",
          "shell.execute_reply.started": "2024-05-07T16:52:19.555471Z",
          "shell.execute_reply": "2024-05-07T16:52:19.577947Z"
        },
        "trusted": true,
        "id": "LReP-sNpbg7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "for _ in iter(train_dataloader):\n",
        "    pass\n",
        "print(count)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-07T16:42:57.167438Z",
          "iopub.execute_input": "2024-05-07T16:42:57.168312Z",
          "iopub.status.idle": "2024-05-07T16:43:01.867228Z",
          "shell.execute_reply.started": "2024-05-07T16:42:57.168283Z",
          "shell.execute_reply": "2024-05-07T16:43:01.866291Z"
        },
        "trusted": true,
        "id": "k8pl0MsFbg7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 3\n",
        "for epoch in range(num_epochs):\n",
        "    train(epoch, num_epochs)\n",
        "    checkpoint(epoch)\n",
        "    outputs, targets, bio_outputs, targets_one_hot_encoded  = validation(epoch, num_epochs)\n",
        "    bio_outputs_list =  [t.tolist() for t in bio_outputs]\n",
        "    print(\"targets shape:\", len(targets[0]))\n",
        "    print(\"output shape: \",len(outputs[0]))\n",
        "    print(\"targets_one_hot_encoded shape: \",type(targets_one_hot_encoded[0]), type(targets_one_hot_encoded))\n",
        "    print(\"bio_output: \",type(bio_outputs[0]), type(bio_outputs))\n",
        "\n",
        "    # change the targets and outputs to list\n",
        "#     print(\"Final output size: \", (outputs))\n",
        "#     print(\"Final target size: \", (targets))\n",
        "#     outputs = np.array(outputs) >= 0.5\n",
        "\n",
        "#     bio_outputs_tensor = torch.tensor(torch.stack(bio_outputs, dim=0), dtype=torch.float32, device=torch.device(device))\n",
        "#     targets_tensor = torch.tensor( targets, dtype=torch.float32, device=torch.device(device))\n",
        "    targets_tensor = torch.tensor( targets_one_hot_encoded, dtype=torch.float32, device=torch.device(device))\n",
        "    outputs_tensor = torch.tensor( outputs, dtype=torch.float32, device=torch.device(device))\n",
        "    targets_flat = targets_tensor.view(-1, 2)\n",
        "    outputs_flat = outputs_tensor.view(-1, 2)\n",
        "    print(outputs_tensor.shape)\n",
        "    print(targets_tensor.shape)\n",
        "    losses  = F.cross_entropy(input=outputs_flat, target=targets_flat.argmax(dim=1), reduction='none')\n",
        "    print(losses)\n",
        "    print(\"Validation Loss: \", (torch.mean(loss_fn(outputs_flat, targets_flat))))\n",
        "#     print(\"Precision: \", precision_score(targets, fin_bio_outputs_list, average=None))\n",
        "#     print(\"Recall: \", recall_score(targets.cpu(), fin_bio_outputs_list, average=None))\n",
        "#     print(\"F1-score: \", f1_score(targets, fin_bio_outputs_list, average=None))\n",
        "#     print(\"Micro Avg F1-score: \", f1_score(targets.cpu(), fin_bio_outputs_list, average='micro'))\n",
        "#     print(\"Macro Avg F1-score: \", f1_score(targets.cpu(), fin_bio_outputs_list, average='macro'))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-07T16:44:46.77904Z",
          "iopub.execute_input": "2024-05-07T16:44:46.779752Z",
          "iopub.status.idle": "2024-05-07T16:47:36.852992Z",
          "shell.execute_reply.started": "2024-05-07T16:44:46.779721Z",
          "shell.execute_reply": "2024-05-07T16:47:36.852098Z"
        },
        "trusted": true,
        "id": "LbtdzGfTbg7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# submission = {'id': [], 'tagged_sentence' : []} # dictionary to store tag predictions\n",
        "# # NOTE ---> ensure that tagged_sentence's corresponing 'id' is same as 'id' of corresponding 'untagged_sentence' in training data\n",
        "# def store_submission(sent_id, tagged_sentence):\n",
        "\n",
        "#     global submission\n",
        "#     submission['id'].append(sent_id)\n",
        "#     submission['tagged_sentence'].append(tagged_sentence)\n",
        "\n",
        "# def clear_submission():\n",
        "#     global submission\n",
        "#     submission = {'id': [], 'tagged_sentence' : []}"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-07T16:47:36.854877Z",
          "iopub.execute_input": "2024-05-07T16:47:36.855307Z",
          "iopub.status.idle": "2024-05-07T16:47:36.859718Z",
          "shell.execute_reply.started": "2024-05-07T16:47:36.855273Z",
          "shell.execute_reply": "2024-05-07T16:47:36.858874Z"
        },
        "trusted": true,
        "id": "r7Umr3Mfbg7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 3\n",
        "# for epoch in range(num_epochs):\n",
        "outputs, targets, bio_outputs, submission, submission_2  = testing()\n",
        "\n",
        "path_to_directory = '/kaggle/working/'\n",
        "pd.DataFrame(list(submission.items()), columns=['Id', 'bio']).to_csv(path_to_directory +'label_submission.csv', index = False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-07T16:52:23.216312Z",
          "iopub.execute_input": "2024-05-07T16:52:23.21713Z",
          "iopub.status.idle": "2024-05-07T16:52:32.335775Z",
          "shell.execute_reply.started": "2024-05-07T16:52:23.217095Z",
          "shell.execute_reply": "2024-05-07T16:52:32.334885Z"
        },
        "trusted": true,
        "id": "WJOdocDlbg7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(submission_2)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-07T16:52:32.337541Z",
          "iopub.execute_input": "2024-05-07T16:52:32.337911Z",
          "iopub.status.idle": "2024-05-07T16:52:32.348077Z",
          "shell.execute_reply.started": "2024-05-07T16:52:32.337877Z",
          "shell.execute_reply": "2024-05-07T16:52:32.347162Z"
        },
        "trusted": true,
        "id": "2o_yIYg_bg7S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}